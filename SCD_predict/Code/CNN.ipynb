{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361364bitpy36condaecf8f48c653943c69a9046fbf3e35959",
   "display_name": "Python 3.6.13 64-bit ('py36': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "410a12a611beedd75eb6276e7d9889a2a7427280b4e30713f0c4338bedf5fc2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 路径、超参数设置\n",
    "nsr_path = '../Dataset/mit-bih-normal-sinus-rhythm-database-1.0.0/'\n",
    "scd_path = '../Dataset/sudden-cardiac-death-holter-database-1.0.0/'\n",
    "ecg_info_save_path = '../Dataset/ecg_info/'\n",
    "hrv_info_save_path = '../Dataset/hrv_info/'\n",
    "scd_fs = 250\n",
    "nsr_fs = 128\n",
    "# preprocess index define: delete 3 records since there SCA don't happen among them and 2 records because of their bad signal quality\n",
    "scd_indexs = ['30','31','32','33','34','35','36','38','41','43','44','45','46','47','48','50','51','52']\n",
    "nsr_indexs = ['16265','16272','16273','16420','16483','16539','16773','16786','16795','17052','17453','18177','18184','19088','19090','19093','19140','19830']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取代表心拍\n",
    "def beat_template_extraction(heart_beats,threshold=0.1):\n",
    "    '''\n",
    "    '''\n",
    "    temp = np.array([heart_beats['0']])\n",
    "    nums = np.array([1],dtype=np.int32)\n",
    "    temp_map = {}\n",
    "    temp_map['0'] = 0\n",
    "    for i in range(1,heart_beats.shape[1]-1):\n",
    "        beat = np.asarray(heart_beats[str(i)])\n",
    "        dist = np.linalg.norm(temp-beat,axis=1)\n",
    "        idx = np.argmin(dist)\n",
    "        if dist[idx] < threshold:\n",
    "            temp[idx] = (temp[idx]*nums[idx]+beat)/(nums[idx]+1)\n",
    "            nums[idx] += 1\n",
    "            temp_map[str(i)] = idx\n",
    "        else:\n",
    "            temp = np.append(temp,[beat],axis=0)\n",
    "            temp_map[str(i)] = nums.shape[0]\n",
    "            nums = np.append(nums,[1])\n",
    "    return temp,nums,temp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(24745, 75)\n(24745,)\n"
     ]
    }
   ],
   "source": [
    "# 构建同一长度的心拍数据集\n",
    "# shapelets 数据集制作\n",
    "# 独立心拍\n",
    "dataset = []\n",
    "labels = []\n",
    "for index in scd_indexs:\n",
    "    heart_beat = pd.read_csv(ecg_info_save_path+index+'_heartbeat_chan0.csv')\n",
    "    temp,nums,temp_map = beat_template_extraction(heart_beat,threshold=0.5)\n",
    "    for i in range(heart_beat.shape[1]-1):\n",
    "        if nums[temp_map[str(i)]]>30:\n",
    "            dataset.append(np.asarray(heart_beat[str(i)][:75],dtype=np.float32))\n",
    "            labels.append(1)\n",
    "for index in nsr_indexs:\n",
    "    heart_beat = pd.read_csv(ecg_info_save_path+index+'_heartbeat_chan0.csv')\n",
    "    temp,nums,temp_map = beat_template_extraction(heart_beat,threshold=0.5)\n",
    "    for i in range(heart_beat.shape[1]-1):\n",
    "        if nums[temp_map[str(i)]]>30:\n",
    "            dataset.append(np.asarray(heart_beat[str(i)][:75],dtype=np.float32))\n",
    "            labels.append(0)\n",
    "dataset = np.asarray(dataset,dtype=np.float32)\n",
    "labels = np.asarray(labels,dtype=np.int32)\n",
    "print(dataset.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([18558, 75])\ntorch.Size([6187, 75])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(dataset,labels,random_state=666)\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "x_test = torch.from_numpy(x_test)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([18558, 1, 1, 75]) torch.Size([6187, 1, 1, 75])\n",
      "/root/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/root/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/root/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/root/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.tensor(x_train,dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 1, x_train.shape[1])\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 1, x_test.shape[1])\n",
    "y_train = y_train.reshape(y_train.shape[0], 1, 1)\n",
    "y_test = y_test.reshape(y_test.shape[0], 1, 1)\n",
    "print(x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一维卷积神经网络模型\n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(cnn_model,self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1,out_channels=10,kernel_size=3,stride=1)\n",
    "        self.max_pool1 = nn.MaxPool1d(kernel_size=3,stride=2)\n",
    "        self.conv2 = nn.Conv1d(10,20,3,1)\n",
    "        self.max_pool2 = nn.MaxPool1d(3,2)\n",
    "        self.conv3 = nn.Conv1d(20,40,3,1)\n",
    "\n",
    "        self.linear1 = nn.Linear(560,70)\n",
    "        self.linear2 = nn.Linear(70,10)\n",
    "        self.linear3 = nn.Linear(10,1)\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1,560)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.sigmoid(self.linear3(x))\n",
    "        return x\n",
    "model = cnn_model(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0.3741, 0.3820, 0.3789, 0.3776, 0.3904, 0.3962, 0.3840, 0.3746,\n",
       "          0.3774, 0.3793, 0.3763, 0.3742, 0.3655, 0.3468, 0.3347, 0.3326,\n",
       "          0.3189, 0.2958, 0.2892, 0.2466, 0.0913, 0.0000, 0.2468, 0.6745,\n",
       "          0.9291, 1.0000, 0.9050, 0.5518, 0.2112, 0.1806, 0.2619, 0.2570,\n",
       "          0.2536, 0.2707, 0.2798, 0.3075, 0.3246, 0.3346, 0.3654, 0.3670,\n",
       "          0.3516, 0.3725, 0.3821, 0.3649, 0.3668, 0.3723, 0.3732, 0.3838,\n",
       "          0.3698, 0.3427, 0.3577, 0.3846, 0.3750, 0.3612, 0.3769, 0.3979,\n",
       "          0.3993, 0.3986, 0.4120, 0.4193, 0.4104, 0.4069, 0.4116, 0.4053,\n",
       "          0.3837, 0.3621, 0.3557, 0.3563, 0.3449, 0.3340, 0.3439, 0.3569,\n",
       "          0.3527, 0.3470, 0.3564]]])"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.4881]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "model(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器和损失函数\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.3567)"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "criterion(torch.Tensor([[0.7]]),torch.Tensor([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s][1,  2000] loss: 0.153\n",
      "[1,  4000] loss: 0.011\n",
      "[1,  6000] loss: 0.016\n",
      "[1,  8000] loss: 0.013\n",
      "[1, 10000] loss: 0.000\n",
      "[1, 12000] loss: 0.028\n",
      "[1, 14000] loss: 0.009\n",
      "[1, 16000] loss: 0.005\n",
      "[1, 18000] loss: 0.009\n",
      " 10%|█         | 1/10 [18:27<2:46:08, 1107.65s/it][2,  2000] loss: 0.017\n",
      "[2,  4000] loss: 0.004\n",
      "[2,  6000] loss: 0.009\n",
      "[2,  8000] loss: 0.013\n",
      "[2, 10000] loss: 0.001\n",
      "[2, 12000] loss: 0.000\n",
      "[2, 14000] loss: 0.005\n",
      "[2, 16000] loss: 0.012\n",
      "[2, 18000] loss: 0.007\n",
      " 20%|██        | 2/10 [36:52<2:27:26, 1105.81s/it][3,  2000] loss: 0.001\n",
      "[3,  4000] loss: 0.006\n",
      "[3,  6000] loss: 0.000\n",
      "[3,  8000] loss: 0.009\n",
      "[3, 10000] loss: 0.000\n",
      "[3, 12000] loss: 0.000\n",
      "[3, 14000] loss: 0.011\n",
      "[3, 16000] loss: 0.000\n",
      "[3, 18000] loss: 0.000\n",
      " 30%|███       | 3/10 [56:15<2:12:03, 1131.97s/it][4,  2000] loss: 0.013\n",
      "[4,  4000] loss: 0.020\n",
      "[4,  6000] loss: 0.000\n",
      "[4,  8000] loss: 0.005\n",
      "[4, 10000] loss: 0.000\n",
      "[4, 12000] loss: 0.000\n",
      "[4, 14000] loss: 0.015\n",
      "[4, 16000] loss: 0.000\n",
      "[4, 18000] loss: 0.004\n",
      " 40%|████      | 4/10 [1:15:48<1:54:48, 1148.13s/it][5,  2000] loss: 0.000\n",
      "[5,  4000] loss: 0.007\n",
      "[5,  6000] loss: 0.000\n",
      "[5,  8000] loss: 0.009\n",
      "[5, 10000] loss: 0.000\n",
      "[5, 12000] loss: 0.000\n",
      "[5, 14000] loss: 0.000\n",
      "[5, 16000] loss: 0.000\n",
      "[5, 18000] loss: 0.008\n",
      " 50%|█████     | 5/10 [1:36:05<1:37:44, 1172.98s/it][6,  2000] loss: 0.003\n",
      "[6,  4000] loss: 0.006\n",
      "[6,  6000] loss: 0.000\n",
      "[6,  8000] loss: 0.000\n",
      "[6, 10000] loss: 0.000\n",
      "[6, 12000] loss: 0.000\n",
      "[6, 14000] loss: 0.000\n",
      "[6, 16000] loss: 0.000\n",
      "[6, 18000] loss: 0.024\n",
      " 60%|██████    | 6/10 [1:57:05<1:20:10, 1202.62s/it][7,  2000] loss: 0.000\n",
      "[7,  4000] loss: 0.003\n",
      "[7,  6000] loss: 0.018\n",
      "[7,  8000] loss: 0.000\n",
      "[7, 10000] loss: 0.000\n",
      "[7, 12000] loss: 0.000\n",
      "[7, 14000] loss: 0.000\n",
      "[7, 16000] loss: 0.000\n",
      "[7, 18000] loss: 0.032\n",
      " 70%|███████   | 7/10 [2:17:35<1:00:35, 1211.71s/it][8,  2000] loss: 0.014\n",
      "[8,  4000] loss: 0.004\n",
      "[8,  6000] loss: 0.005\n",
      "[8,  8000] loss: 0.000\n",
      "[8, 10000] loss: 0.000\n",
      "[8, 12000] loss: 0.000\n",
      "[8, 14000] loss: 0.002\n",
      "[8, 16000] loss: 0.006\n",
      "[8, 18000] loss: 0.003\n",
      " 80%|████████  | 8/10 [2:38:25<40:47, 1223.65s/it]  [9,  2000] loss: 0.000\n",
      "[9,  4000] loss: 0.006\n",
      "[9,  6000] loss: 0.000\n",
      "[9,  8000] loss: 0.000\n",
      "[9, 10000] loss: 0.000\n",
      "[9, 12000] loss: 0.000\n",
      "[9, 14000] loss: 0.009\n",
      "[9, 16000] loss: 0.000\n",
      "[9, 18000] loss: 0.000\n",
      " 90%|█████████ | 9/10 [2:59:20<20:33, 1233.63s/it][10,  2000] loss: 0.000\n",
      "[10,  4000] loss: 0.006\n",
      "[10,  6000] loss: 0.000\n",
      "[10,  8000] loss: 0.013\n",
      "[10, 10000] loss: 0.000\n",
      "[10, 12000] loss: 0.000\n",
      "[10, 14000] loss: 0.006\n",
      "[10, 16000] loss: 0.000\n",
      "[10, 18000] loss: 0.000\n",
      "100%|██████████| 10/10 [3:20:05<00:00, 1200.56s/it]time = 200m: 5s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练过程\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "start = time.time()\n",
    "for epoch in tqdm(range(10)):\n",
    "    running_loss = 0\n",
    "    for i, input_data in enumerate(x_train, 0):\n",
    "        # print(input_data.shape)\n",
    "        label = y_train[i]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_data)\n",
    "        #print(outputs)\n",
    "        #print(label)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %0.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('time = %2dm:%2ds' % ((time.time() - start)//60, (time.time()-start)%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'./cnn_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/root/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "res = np.zeros(x_test.shape[0])\n",
    "for idx,test in enumerate(x_test):\n",
    "    res[idx] = float(model(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.30000001192092896"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_res = res\n",
    "new_res[np.where(new_res>0.5)[0]] = 1\n",
    "new_res[np.where(new_res<=0.5)[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6187,)"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "np.where(new_res==a)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6187,)"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "a.shape"
   ]
  }
 ]
}